configfile: "config.yaml"

from itertools import chain
from functools import partial
import tempfile
from os.path import basename,dirname
import os.path
import yaml
from subprocess import Popen, PIPE
from scripts.common import detect_reads, fill_default_values

#Config parameters
fill_default_values(config)


# -------- Ressources ----------------
THREADS = config["threads"]
# -------- Data -----------------
IN = config["data"]
READ_LENGTH = config["read_length"]
# -------- Sources -----------------
SCRIPTS = config["scripts"]
DSCRIPTS = config["desman"]["dscripts"]
SOFT = config["spades_tools"]
# -------- COGs -----------------
COG_DB = config["cog_database"]
if basename(COG_DB)!="Cog":
    COG_DB += '/Cog'
SCG_DATA = config["scg_data"]
COG_FILE=SCG_DATA+"/scg_cogs_to_run.txt"
# -------- Assembly -----------------
ASSEMBLER = config["assembly"]["assembler"]
ASSEMBLER_DIR = config["assembly"]["dir"]
ASSEMBLY_K = config["assembly"]["k"][-1]
# -------- Bayesgraph -----------------
MIN_ORF_NUMBER_TO_RUN_A_BIN=config["bayespaths"]["min_orf_number_to_run_a_bin"] # if there is not enough COG left we don't try to resolve strain in that bin
MERGE_VALUE=config["bayespaths"]["min_orf_number_to_merge_bins"] # if there's too many COG in common between bins we mergge them 
MERGE_THRESHOLD=config["bayespaths"]["percent_unitigs_shared"] # minimum number of unitigs shared between graph to consider there is a link 
BAYESPATHS_DIR = config["bayespaths"]["dir"]
BAYESPATHS_G = config["bayespaths"]["nb_strains"]
BAYESPATHS_NMF_RUNS = config["bayespaths"]["nmf_runs"]
BAYESPATHS_MAX_GITER = config["bayespaths"]["max_giter"]
BAYESPATHS_OPTIONAL_ARGS = config["bayespaths"]["optional_args"]
# -------- Binning -----------------
MIN_CONTIG_SIZE = config["concoct"]["contig_size"]

BINNER = config["binner"]
MIN_CONTIG_SIZE = config[BINNER]["contig_size"]
FRAGMENT_SIZE = config["concoct"]["fragment_size"]
BINMULTIPLIER = config["concoct"]["bin_multiplier"]
BINMAX = config["concoct"]["bin_max"]

MAG_QUAL_THRESHOLD = config["mag_quality_threshold"]
# -------- Desman -----------------
DESMAN_HAPLOTYPE_NB = config["desman"]["nb_haplotypes"]
DESMAN_REPEAT = config["desman"]["nb_repeat"]
MIN_COV_DESMAN = config["desman"]["min_cov"]
ANNOTATED_COGS=[s.rstrip() for s in open(COG_FILE)]
# -------- Evaluation -----------------
REFDATA=config["evaluation"]["genomes"]
EVAL_SCRIPTS=config["evaluation"]["scripts"]
EVAL_RUN=config["evaluation"]["execution"]
# -------- Results -----------------
RES_SCRIPTS = SCRIPTS+"/results"
GTDB = config["gtdb_path"]

#Autodetect samples and their reads

def extended_glob(pattern):
    process = Popen(['bash -O extglob -c " ls -d %s/ "'%pattern], stdout=PIPE, stderr=PIPE,shell=True)
    List_path=[element[:-1] for element in process.communicate()[0].decode("utf-8").split("\n") if element]
    return [path for path in List_path if basename(path)!="multiqc_data"]

# samples is a list of the samples names founds, directories names are the names of the samples
if not isinstance(config["samples"], list):
    config["samples"] = [config["samples"]]
SAMPLES = [basename(path) for sample in config["samples"] for path in extended_glob(IN+"/"+sample)]
# print(config["samples"])
# sample_reads is a dictionary of samples to reads files
SAMPLE_READS = dict(map(lambda sample: (sample, detect_reads(os.path.join(IN, sample))), SAMPLES))
# sample_reads is a dictionary of samples to reads files
Files_nb = [sample for sample,list_file in SAMPLE_READS.items() if len(list_file)!=2]
if Files_nb :
    raise WorkflowError("Samples folder : "+" - ".join(Files_nb)+"  does not have exactly 2 reads files, you may have more or less than 2, files recognised as reads files are the following : .fastq, .fastq.gz, .fq, .fq.gz, .fa, .fa.gz, .fasta, .fasta.gz. You may also want to check the regular expression you used to select samples")

# replace_extension, change file name so that trimmed version is taken for assembly/mapping..etc instead of initials samples
R1={sample:list_reads[0] for sample,list_reads in SAMPLE_READS.items()}
R2={sample:list_reads[1] for sample,list_reads in SAMPLE_READS.items()}


def samples_yaml(samples):
    libs = []
    for sample in samples:
        info = {}
        info["left reads"] = [R1[sample]]
        info["right reads"] = [R2[sample]]
        info["type"] = "paired-end"
        info["orientation"] = "fr"
        libs.append(info)
    return yaml.dump(libs, default_style='"', default_flow_style=False)

def is_fastq(wildcards):
    for ext in {".fastq", ".fq", ".fastq.gz", "fq.gz"}:
        if SAMPLE_READS[wildcards.sample][0].endswith(ext):
            return True
    return False


rule bowtie_index:
    input:   "{path}/{ref}.fasta"
    output:  touch("{path}/{ref}/index.done")
    params:  "{path}/{ref}/index"
    threads: THREADS
    log:     "{path}/{ref}/index.log"
    message: "Building bowtie index for {input}"
    shell:   "bowtie2-build {input} {params} --threads {THREADS} --large-index &> {log}"

#consider ignoring reads aligned without their pairs
rule bowtie_align:
    input:   left = lambda w:R1[w.sample],
             right = lambda w:R2[w.sample],
             index="{path}/index.done"
    output:  "{path}/{sample}.bam"
             #temp("{path}/{group,(sample|group)\d+}.bam")
    params:  flag=lambda w: "-q" if is_fastq(w) else "-f",
             left=lambda w: ",".join(expand("{l}", l=R1[w.sample])),
             right=lambda w: ",".join(expand("{r}", r=R2[w.sample])),
             index="{path}/index",
             align="",#"--no-unal --maxins 1000 --n-ceil 0,0 --np 100500",
             view=""#"-q 10"
    threads: THREADS
    log:     "{path}/{sample}.bowtie.log"
    message: "Aligning reads of {wildcards.sample} onto {params.index} with bowtie"
    shell:
        "bowtie2 -x {params.index} {params.flag} -p {threads} {params.align} -1 {params.left} -2 {params.right} 2> {log}"
        " | samtools view -bh {params.view} - > {output}"

rule samtools_sort:
    input:
        "{path}/{name}.bam"
    output:
        "{path}/{name}.sorted.bam"
        #temp("{path}/{name}.sorted.bam")
    threads: THREADS
    log:
        "{path}/{name}.sort.log"
    message: "Sorting {input}"
    run:
        shell("samtools sort --threads %d -T {wildcards.path}/{wildcards.name}.tmp -O bam -o {output} {input} &> {log}" % (threads - 1))

rule index_fasta:
    input: "{path}.fasta"
    output: "{path}.fasta.fai"
    shell: "samtools faidx {input}"

rule index_samples_bam:
    input:   "{path}.sorted.bam"
    output:  "{path}.sorted.bam.bai"
    message: "Indexing bam file {wildcards.path}.sorted.bam"
    shell:   "samtools index {input}"

def read_selected_bins(fn):
    assert os.path.exists(fn)
    print("Reading selected bins from %s" % fn)
    return [line.strip() for line in open(fn).readlines()]


def read_mags(fn):
    assert os.path.exists(fn)
    print("Reading selected MAGs from %s" % fn)
    return ["Bin_%s" % line.rstrip() for line in open(fn).readlines()]
