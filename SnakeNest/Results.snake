include: "Common.snake"
import os
import glob
import numpy as np 
import seaborn as sns
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
from collections import defaultdict
from os.path import dirname, basename
from Bio.SeqIO.FastaIO import SimpleFastaParser as sfp


SELECTED_MAGS = sorted([mag.rstrip() for mag in open("bayespaths/selected_bins.txt")])
ALL_MAGS = {"Bin_%s"%line.rstrip() for line in open("binning/list_mags.tsv")}
# there is the number of haplotype in the file name, this is hard for snakemake, so I need to get it beforehand
MAG_TO_PATHS = {mag:glob.glob("bayespaths/%s/*Haplo_*_path.txt"%mag)[0] for mag in SELECTED_MAGS}
MAG_TO_HAPLO = {mag:glob.glob("bayespaths/%s/*Haplo_*.fa"%mag)[0] for mag in SELECTED_MAGS}
MAG_TO_HAPLO_NB = {mag:len({header.split('_')[1] for header,_ in sfp(open(file))}) for mag,file in MAG_TO_PATHS.items()}
# number of cogs is dependant on mags 
MAG_TO_COGS = {mag:sorted({header.split('_')[0] for header,seq in sfp(open(MAG_TO_PATHS[mag]))}) for mag in SELECTED_MAGS}

#--------------------Snakemake ------------------------
#### specify wanted outputs

# summary file :
TODO_LIST = ["results/summary.tsv"]

# generate colored SCG graphs, one by mag
TODO_LIST.append(expand("results/{mag}/graph/joined_SCG_graph.gfa",mag=SELECTED_MAGS))

# generate haplotype tree, one for each mag
TODO_LIST.append(expand("results/{mag}/haplotypes_tree.nwk",mag=SELECTED_MAGS))
TODO_LIST.append(expand("results/{mag}/tmp//dist_mat.tsv",mag=SELECTED_MAGS))

# generate haplotype coverage
TODO_LIST.append("results/haplotypes_coverage.pdf")
TODO_LIST.append("results/mag_and_haplo_cov.tsv")

# fig : haplo_vs_magcov
TODO_LIST.append("results/haplo_nb_vs_mag_cov.png")

# if GTDB is not empty, generate a gtdb tree of mags 
if GTDB!="":
    TODO_LIST += ["results/gtdb/gtdbtk.bac120.summary.tsv"]

rule all: 
    input: TODO_LIST

# ----------------- global summary -----------------
# Adress the following --> nb_strains, coverage of each strains, total mag coverage, divergence, nb mags, nb bins, quality of assembly  

def get_taxa(file,mag_to_classification={}):
    if os.path.getsize(file)!=0:
        with open(file) as handle : 
            header = next(handle)
            for line in handle :
                splitline = line.rstrip().split("\t")
                mag = splitline[0]
                classification = [ taxa.split("__")[1] for taxa in splitline[1].split(';')]
                classification = [taxa if taxa else '/' for taxa in classification]
                Red = splitline[-2]
                if Red == 'N/A' :
                    Red = 1
                mag_to_classification[mag] = "\t".join([str(Red)]+classification)
    return mag_to_classification

generate_summary_input = ["profile/assembly.fasta"]+(GTDB!="")*["results/gtdb/gtdbtk.bac120.summary.tsv","results/gtdb/gtdbtk.ar122.summary.tsv"]

rule generate_summary:
    input: generate_summary_input
    output: file = 'results/summary.tsv'
    run:
        # information on assembly
        with open(output["file"],"w") as handle:
            handle.write("#-------- assembly statistics --------\n")
        shell("{RES_SCRIPTS}/contig-stats.pl %s>>{output}"%input[0])
        handle = open(output["file"],"a")
        # information on mags
        handle.write("#--------     MAGs summary    --------\n")
        mag_nb = len(ALL_MAGS)
        haplo_nb = sum(MAG_TO_HAPLO_NB.values())
        handle.write("mags number: %s\tresolved haplotypes: %s\n"%(mag_nb,haplo_nb))
        if GTDB:
            bac_nb = sum([1 for line in open(input[1])])
            arc_nb = sum([1 for line in open(input[2])])
            handle.write("bacteria: %s\tarcheae: %s\n"%(bac_nb,arc_nb))
            # add taxonomy and RED value
            handle.write("#-------- MAGs gtdb taxonomy  --------\n")
            handle.write("mag\tnb_haplo\tRED\tDomain\tPhylum\tClass\tOrder\tfamilly\tgenus\tstrain\n\n")
            mag_to_taxa = get_taxa(input[1])
            mag_to_taxa = get_taxa(input[2]) # worst non readable trick I know so far, using an empty dict as defaultvalue will cary over multiple call.... https://stackoverflow.com/questions/26320899/why-is-the-empty-dictionary-a-dangerous-default-value-in-python 
            handle.writelines("%s\t%s\t%s\n"%(mag,MAG_TO_HAPLO_NB[mag],taxo) for mag,taxo in mag_to_taxa.items())

# ----------------- coverage of mag/strains -----------------
##### get SCG coverages
# keep only cogs corresponding to scg
rule get_scg_coverage:
    input:   bed = "annotation/assembly.bed",
             scg_annot = "annotation/Cogs_filtered.tsv"
    output:  bed="annotation/scg.bed",
             scg_to_orfs = "annotation/map_scgs_to_orfs.tsv"
    run : 
        scgs = {line.rstrip() for line in open(COG_FILE)}
        # get scg to orfs mapping
        scg_to_orfs = defaultdict(list)
        for line in open(input["scg_annot"]):
            splitlines = line.split("\t")
            if splitlines[1] in scgs:
                scg_to_orfs[splitlines[1]].append(splitlines[0])
        # select only orfs relevant to scg cogs
        orfs = {val for values in scg_to_orfs.values() for val in values}
        with open(output["bed"],"w") as handle:
            for line in open(input["bed"]):
                if line.rstrip().split('\t')[3] in orfs:
                    handle.write(line)
        # output scg to orf mapping : 
        with open(output["scg_to_orfs"],"w") as handle:
            handle.writelines("%s\t%s\n"%(scg,"\t".join(orfs)) for scg,orfs in scg_to_orfs.items())

# extract cov for selected features
rule bedtools:
    input:   bam="profile/assembly/{sample}.sorted.bam",
             bed="annotation/scg.bed"
    output:  "profile/scgs/{sample}.orfs.cov"
    log:      "profile/scgs/{sample}.orfs.log"
    resources:
        memG=400
    shell:   "bedtools coverage -a {input.bed} -b {input.bam} -mean > {output} 2>{log} "

# collate all files  
rule coverage:
    input:   expand("profile/scgs/{sample}.orfs.cov",sample = SAMPLES)
    output:  "profile/coverage_scg_orfs.tsv"
    shell : "{SCRIPTS}/collate_coverage.py -o {output} -l {input} "

# aggregate orf coverage following inputed scg to orfs mapping
rule generate_profile:
    input:  cov="profile/coverage_scg_orfs.tsv",
            map="annotation/map_scgs_to_orfs.tsv"
    output: "profile/cov_scg.tsv"
    shell:  "{SCRIPTS}/Extract_gene_profile.py {input.map} {input.cov} {output}"

# just take the median of cov_scg.tsv
rule median_SCG_cov:
    input:  cov="profile/cov_scg.tsv",
    output: "profile/Normalisation.tsv"
    run: 
        List_profile = []
        # compute median of SCG       
        with open(input.cov) as handle:
            samples = next(handle).rstrip().split("\t")[1:]
            List_profile = [[float(element) for element in line.rstrip().split("\t")[1:]] for line in handle]
        scg_norm=np.median(List_profile, axis=0)
        # output
        with open(output[0],"w") as handle:
            handle.write("Normalisation\t"+"\t".join(samples)+"\n")
            handle.write("median_scg\t"+"\t".join(map(str,scg_norm))+"\n")

#### get selected all mag coverage
def format_row(header,line):
    row = header
    for el in line:
        row += "\t{:.2g}".format(el)
    return row+"\n"
def matrix_write(matrix,file_name,col_names,row_names):
    with open(file_name,"w") as handle:
        handle.write("/\t%s\n"%"\t".join(col_names))
        handle.writelines(format_row(row_names[index],line) for index,line in enumerate(matrix))


rule mag_coverage:
    input: cluster = "binning/clustering_gt1000_merged.csv",
           cov = "profile/split/coverage.tsv",
           bed = "profile/split.bed"
    output: cov = "profile/mag_cov.tsv"
    run:
        # get list of contigs corresponding ot selected mags 
        mags = {mag.replace("Bin_","") for mag in ALL_MAGS}
        mag_to_contigs = defaultdict(list)
        with open(input["cluster"]) as handle:
            _=next(handle)
            for line in handle:
                contig,mag = line.rstrip().split(",")
                if mag in mags:
                    mag_to_contigs["Bin_%s"%mag].append(contig)
        # translate contig to split contigs, so that we don't need to get cov for non split contigs
        contigs = {contig for cntgs in mag_to_contigs.values() for contig in cntgs}
        contigs_to_splits = defaultdict(list)
        split_to_len = {}
        for line in open(input["bed"]):
            contig,start,end,split = line.rstrip().split("\t")
            if contig in contigs:
                split_to_len[split]=np.abs(int(end)-int(start))
                contigs_to_splits[contig].append(split)
        # read coverage file and only look at selected splits
        # in the mean time, fill a matrix of number of nucleotide per mag
        splits_to_mag = {split:mag for mag,contigs in mag_to_contigs.items() for contig in contigs for split in contigs_to_splits[contig]}
        mag_to_len = defaultdict(int)
        with open(input["cov"]) as handle:
            sorted_samples = next(handle).rstrip().split('\t')[1:]
            sorted_mags = sorted(ALL_MAGS)
            nuc_matrix = np.zeros((len(sorted_mags),len(sorted_samples)))
            for line in handle:
                splitline = line.rstrip().split("\t")
                if splitline[0] in splits_to_mag:
                    # get things
                    mag = splits_to_mag[splitline[0]]
                    row_index = sorted_mags.index(mag)
                    split_len = split_to_len[splitline[0]]
                    profile = split_len*np.array([float(val) for val in splitline[1:]])
                    # store things
                    nuc_matrix[row_index,:] += profile
                    mag_to_len[mag] += split_len
        # get mag coverage, from mag mapped nucleotides
        sorted_mag_len = np.array([mag_to_len[mag] for mag in sorted_mags])
        cov_matrix = (nuc_matrix.T/sorted_mag_len).T
        # output result:
        matrix_write(cov_matrix,output["cov"],sorted_samples,sorted_mags)

def load_matrix(file,sample_order=None,strain_order=None) :
    with open(file) as handle :
        header = next(handle).rstrip().split("\t")[1:]
        strains = []
        matrix = []
        for line in handle : 
            splitlines = line.rstrip().split("\t")
            strains.append(splitlines[0])
            matrix.append(list(map(float,splitlines[1:])))
    matrix = np.array(matrix)
    if sample_order :
        reorder_samples = [header.index(sample) for sample in sample_order]
        reorder_strain = [strains.index(strain) for strain in strain_order]
        return matrix[:,reorder_samples][reorder_strain,:]
    else : 
        return matrix,header,strains

def convert_intensity_to_kcov(mat):
    return mat*READ_LENGTH

def convert_kcov_to_cov(mat):
#Ck*Read_length = C *(Read_length-kmer_size+1)
    return mat*READ_LENGTH/float(READ_LENGTH-ASSEMBLY_K+1)

def load_intensity(file,mag):
    with open(file) as handle:
        header = [int(val) for val in next(handle).rstrip().split(",")[1:]]
        sorted_haplo = [] 
        matrix = []
        for line in handle: 
            temp = [0 for ite in range(len(SAMPLES))]
            splitlines = line.rstrip().split(",")
            sorted_haplo.append("%s_haplo_%s"%(mag,splitlines[0]))
            for index,val in enumerate(splitlines[1:]):
                temp[header[index]]=float(val)
            matrix.append(temp)
        matrix = np.array(matrix)
        matrix = convert_intensity_to_kcov(matrix)
        matrix = convert_kcov_to_cov(matrix)
        return matrix,sorted_haplo


# get one matrix with mag coverage and strains coverages
rule create_unique_coverage_matrix:
    input: cov = "profile/mag_cov.tsv",
           norm = "profile/Normalisation.tsv"
    output: cov = 'results/mag_and_haplo_cov.tsv'
    run:
        mag_matrix,samples,mags = load_matrix(input["cov"])
        norm,_,_ = load_matrix(input["norm"])
        mag_matrixN = mag_matrix/norm
        with open(output["cov"],"w") as handle: 
            handle.write("/\t%s\n"%"\t".join(SAMPLES))
            for mag in SELECTED_MAGS:
                handle.write(format_row(mag,mag_matrixN[mags.index(mag)]))
                file = "bayespaths/%s/%sF_Intensity.csv"%(mag,mag) 
                haplo_mat,sorted_haplo = load_intensity(file,mag)
                haplo_matN = haplo_mat/norm
                for index,line in enumerate(haplo_matN):
                    handle.write(format_row(sorted_haplo[index],line))


# --------------plot nb strain VS total mag coverage --------------
rule plot_fig :
    input : cov = "profile/mag_cov.tsv"
    output: fig = "results/haplo_nb_vs_mag_cov.png"
    run: 
        cov,samples,mags = load_matrix(input['cov'])
        covtot = cov.sum(1)
        strain_nb = [len({header.split('_')[1] for header,seq in sfp(open(MAG_TO_HAPLO[mag]))}) for mag in mags]
        ax = sns.regplot(x=covtot, y=strain_nb, logx=True)
        plt.title("Haplotype number versus total mag coverage")
        plt.title("Haplotype number versus total mag coverage")
        plt.xlabel("Total mag coverage over all samples")
        plt.ylabel("Number of haplotypes resolved for each mag")
        plt.xscale("log")
        ax.set_xticks([10**powr for powr in range(int(np.log10(min(covtot))),int(np.log10(max(covtot)))+1)])
        ax.set_yticks([int(nb) for nb in range(min(strain_nb),max(strain_nb)+1)])
        plt.savefig(output['fig'])


# ----------------- barplot strain coverage -----------------
rule coverage_barplot:
    input: cov = "bayespaths/{mag}/{mag}F_Intensity.csv",
           var = "bayespaths/{mag}/{mag}F_varIntensity.csv",
           norm = "profile/Normalisation.tsv"
    output: "results/{mag}/haplotypes_coverage.pdf"
    shell: "{RES_SCRIPTS}/GGBarPlotAbundances.R {input.cov} {input.var} {input.norm} {READ_LENGTH} {output}"

rule cat_pdf:
    input: expand("results/{mag}/haplotypes_coverage.pdf",mag=SELECTED_MAGS)
    output: "results/haplotypes_coverage.pdf"
    shell: "pdfunite {input} {output}"

# ----------------- one denovo tree by mag -----------------
rule split_by_cog:
    input: haplo = [MAG_TO_HAPLO[mag] for mag in SELECTED_MAGS]
    output: tmp = ["results/%s/tmp/%s.fna"%(mag,cog) for mag in SELECTED_MAGS for cog in MAG_TO_COGS[mag]]
    run : 
        # for each selected mag get the low resolution SCG sequence
        for file in input["haplo"]:
            mag = basename(dirname(file))
            cog_to_strain_seq = defaultdict(lambda:{})
            # get resolved haplotypes sequences
            for header,seq in sfp(open(file)):
                cog,strain_nb = header.split("_")
                strain = "%s_haplo_%s"%(mag,strain_nb)
                cog_to_strain_seq[cog][strain] = seq
            # get the low resolution SCG sequence
            file2 = "subgraphs/bin_merged/%s/SCG.fna"%mag
            for header,seq in sfp(open(file2)):
                cog = header.split(" ")[1]
                cog_to_strain_seq[cog][mag] = seq
            for cog,strain_to_seq in cog_to_strain_seq.items(): 
                with open("results/%s/tmp/%s.fna"%(mag,cog),"w") as handle:
                    handle.writelines(">%s\n%s\n"%(strain,seq) for strain,seq in strain_to_seq.items())


rule run_mafft:
    input: scg="results/{mag}/tmp/{cog}.fna",
    output: msa="results/{mag}/tmp/{cog}.msa_temp",
    log: "results/{mag}/tmp/{cog}_mafft.log"
    threads:1000
    shell: """
    mafft --thread {threads} {input.scg} > {output.msa} 2>{log}
    """

# sometimes scg fragments from low resolution assembly are just too small and this throw a seg fault with trimal, so we need to filter this
rule check_length :
    input: msa = "{path}.msa_temp"
    output: msa = "{path,.*(?!_trim).*}.msa"
    run:
        with open(output['msa'],"w") as handle:
            for header,seq in sfp(open(input["msa"])):
                nb = seq.count("-")
                if nb/float(len(seq))<=0.90:
                    handle.write(">%s\n%s\n"%(header,seq))

rule trimal :
    input: "{path}/{COG}.msa"
    output: "{path}/{COG}_trim.msa"
    shell: "trimal -in {input} -out {output} -gt 0.9 -cons 60"

rule cat_cogs :
    input : trim = lambda w:["results/%s/tmp/%s_trim.msa"%(w.mag,cog) for cog in MAG_TO_COGS[w.mag]]
    output : cat = "results/{mag}/tmp/concatenated_cogs.msa"
    run : 
        mag = wildcards.mag
        strain_to_cog_seq = defaultdict(lambda:{})
        cog_to_strain_seq = defaultdict(lambda:{})
        for file in input["trim"]:
            cog = basename(file).replace("_trim.msa","")
            for strain,seq in sfp(open(file)):
                strain_to_cog_seq[strain][cog] = seq
                cog_to_strain_seq[cog][strain] = seq
        sorted_cogs = sorted(cog_to_strain_seq.keys())
        sorted_strains = sorted(strain_to_cog_seq.keys())
        mean_cog_len = {cog:int(np.mean([len(seq) for seq in dict_strain.values()])) for cog,dict_strain in cog_to_strain_seq.items()}
        # get strain concatenated sequence
        strain_to_seq = defaultdict(str)
        for strain in sorted_strains:
            for cog in sorted_cogs: 
                if cog in strain_to_cog_seq[strain]:
                    strain_to_seq[strain]+=strain_to_cog_seq[strain][cog]
                else :
                    # deal with missing cogs
                    strain_to_seq[strain]+=mean_cog_len[cog]*"-"
        with open(output["cat"],"w") as handle : 
            handle.writelines(">%s\n%s\n"%(strain,seq) for strain,seq in strain_to_seq.items())

rule Launch_FastTree:
    input: "{path}/tmp/concatenated_cogs.msa"
    output: "{path}/haplotypes_tree.nwk"
    log: "{path}/tmp/fastree.log"
    shell:"""
    FastTreeMP -nt -gtr < {input} 2> {log} > {output}
    """

#--- look at dist between strains ---
rule COG_dist:
    input: "{path}/concatenated_cogs.msa"
    output:"{path}/dist.tsv"
    log:"{path}_dist.log"
    shell: "distmat -nucmethod 0 {input} -outfile {output} &>{log}" 

rule parse_emboss_dist:
    input: dist = "{path}/dist.tsv"
    output: dist = "{path}/dist_mat.tsv"
    run:
        with open(input["dist"]) as handle:
            for nb in range(8):
                header=next(handle).rstrip().replace(" ","").split("\t")
            nb_genomes=len(header)-header.count("")
            Mat=np.zeros((nb_genomes,nb_genomes))
            sorted_gen=[]
            for ind,line in enumerate(handle):
                splitline=line.rstrip().split("\t")[1:]
                genome=splitline[-1].split(" ")[0]
                sorted_gen.append(genome)
                Mat[ind,ind:]=np.array([float(el) for el in splitline[ind:-2]])
            Mat+=Mat.T
        matrix_write(Mat,output["dist"],genome,genome)




# ----------------- colored .gfa -----------------
# keep the same color for the same strain --> generate 1 graph by mag by cog
rule color_graph :
    input : gfa = "subgraphs/bin_merged/{mag}/simplif/{cog}.gfa",
            path = lambda w:MAG_TO_PATHS[w.mag]
    output : "results/{mag}/graph/cogs/{cog}_color.gfa"
    params : kmer = 77#config["assembly"]["k"]
    shell : """
    {RES_SCRIPTS}/color_graph.py {input.gfa} -p {input.path} {output}.temp
    {SCRIPTS}/Bandage_Cov_correction.py {output}.temp {params.kmer} {output}
    rm {output}.temp
    """

# also merge .gfa so that we can see some sort of continuity
rule join_graphs :
    input : path = lambda w:MAG_TO_PATHS[w.mag],
            gfa = lambda w:["results/%s/graph/cogs/%s_color.gfa"%(w.mag,cog) for cog in  MAG_TO_COGS[w.mag]]
    output : "results/{mag}/graph/joined_SCG_graph.gfa"
    shell : "{RES_SCRIPTS}/join_graphs.py {input.path} {output} -l {input.gfa}"


# ----------------- gtdb tree  -----------------
rule mag_directory:
    input: clustering = "binning/clustering_gt1000_merged.csv",
           assembly = "profile/assembly.fasta",
           mag_list = "binning/list_mags.tsv"
    output: done = "binning/MAGs/done"
    shell: """
           {SCRIPTS}/Split_fasta_by_bin.py {input.assembly} {input.clustering} $(dirname {output.done}) -l $(cat {input.mag_list})
           touch {output.done}
           """

def which(pgm):
    # https://stackoverflow.com/questions/9877462/is-there-a-python-equivalent-to-the-which-command
    path=os.getenv('PATH')
    for p in path.split(os.path.pathsep):
        p=os.path.join(p,pgm)
        if os.path.exists(p) and os.access(p,os.X_OK):
            return p

rule get_gtdb_database:
    output: db_done="results/gtdb/done",
    run:
        # get strong env path
        path = which("gtdbtk")
        env_path = path.split("/bin/gtdbtk")[0]
        # just check it is not empty
        if os.listdir(GTDB)==[".empty"]:
            # create a copy of download script with new dl directory
            dl_script = which("download-db.sh")
            new_dl_script = dirname(dl_script)+"/download-db2.sh"
            shell("cp %s %s"%(dl_script,new_dl_script))
            shell("sed -i 's/${GTDBTK_DATA_PATH}/%s/g' %s"%(GTDB,new_dl_script))
            shell("download-db2.sh")
        # overwrite default database location
        export_path = env_path+"/etc/conda/activate.d/gtdbtk.sh"
        with open(export_path,"w") as handle:
            handle.write("export GTDBTK_DATA_PATH=%s/\n"%GTDB)
        shell("source %s"%export_path)
        # signal process went is done
        shell("touch %s"%output["db_done"])

rule gtdb:
    input: done = "binning/MAGs/done",
           db_done = '%s/done'%GTDB
    output: bac = "results/gtdb/gtdbtk.bac120.summary.tsv",
            arc =  "results/gtdb/gtdbtk.ar122.summary.tsv"
    threads: 50
    shell: """
            gtdbtk classify_wf --cpus {threads} --genome_dir $(dirname {input.done})  --out_dir $(dirname {output.bac}) --extension .fasta
            touch {output.bac}
            touch {output.arc}
           """


