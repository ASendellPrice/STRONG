
#rule all_local: 
#   input: cov="binning/C10K_coverage.tsv",
#          bins="binning/clustering_gt500.csv",
#          touch="binning/bin_creation.done"


#cut contigs by ORFs and get the bed file
rule cut_contigs:
    input:  fa="profile/assembly.fasta",
            gff="annotation/assembly.gff"
    output: contig="profile/split.fasta",
            contig_bed="profile/split.bed"
    shell:  "{SCRIPTS}/Use_orf_to_cut.py {input.fa} {input.gff} {output.contig_bed} -c {FRAGMENT_SIZE}> {output.contig}"

# ---- use bedtool to compute coverage  ----------------------------------------------------
rule bedtools_split_cov:
    input:   sample="profile/assembly/{sample}.sorted.bam",
             bed="profile/split.bed"
    output:  "profile/split/{sample}.cov"
    log:     "profile/split/{sample}.log"
    shell:   "bedtools coverage -a {input.bed} -b {input.sample} -mean > {output} 2>{log} "

# ---- use a awk onliner to regroup all coverages into a unique file -----------------------
rule coverage:
    input:   expand("profile/split/{sample}.cov", sample = SAMPLES)
    output:  "profile/split/coverage.tsv"
    shell : """
            echo -e "contig\t""$(ls {input} | cut -f1 -d "." | rev | cut -f1 -d "/" |rev | tr "\n" "\t" | sed 's/\t$//')"> {output}
            awk 'NR==FNR{{Matrix_coverage[1,FNR]=$4}}FNR==1{{f++}}{{Matrix_coverage[f+1,FNR]=$5}}END{{for(x=1;x<=FNR;x++){{for(y=1;y<ARGC+1;y++){{if(y<ARGC){{printf("%s\t",Matrix_coverage[y,x])}}if(y==ARGC){{printf("%s",Matrix_coverage[y,x]);print""}}}}}}}}' {input} >>{output}"""

# We take a look at all the SCG, take the median over the 36 of them, and multiply that by 10
rule initial_quantity_of_bins:
    input: "annotation/SCG.fna"
    output:"binning/ini_bins.cnt"
    shell: "{SCRIPTS}/get_num_bin_ini.py {input}>{output}"

#  concoct 
rule concoct:
    input:   cov="profile/split/coverage.tsv",
             fasta="profile/split.fasta",
             bin_cnt="binning/ini_bins.cnt"
    output:  bins="binning/clustering_gt%d.csv" % MIN_CONTIG_SIZE,
             data="binning/original_data_gt%d.csv" % MIN_CONTIG_SIZE
    log :   "binning/concoct.logs"
    threads: 1000
    shell:   """
             concoct --coverage_file {input.cov} --composition_file {input.fasta} -b $(dirname {output.bins}) -c $(<{input.bin_cnt}) -l {MIN_CONTIG_SIZE} -t {threads} &>{log} 
             """

# path=binning
rule refine:
    input:  bins="{path}/clustering_gt%d.csv" % MIN_CONTIG_SIZE,
            SCG="annotation/SCG.fna",
            data="{path}/original_data_gt%d.csv" % MIN_CONTIG_SIZE
    output: R=temp("{path}/clustering_gt%dR.csv") % MIN_CONTIG_SIZE,
            table="{path}/clustering_gt%d_SCG_table.csv" % MIN_CONTIG_SIZE,
            bins_R="{path}/clustering_refine.csv",
            table_R="{path}/clustering_gt%d_SCG_table_R.csv" % MIN_CONTIG_SIZE
    log:    "{path}/refine.log"
    threads: 1000
    shell:  """ 
            {SCRIPTS}/SCG_in_Bins.py {input.bins} {input.SCG} -t {output.table}
            sed '1d' {input.bins}  > {output.R}
            cd binning
            concoct_refine ../{output.R} ../{input.data} ../{output.table} -t {threads} &> ../{log}
            cd -
            {SCRIPTS}/SCG_in_Bins.py {output.bins_R} {input.SCG} -t {output.table_R}
            """

rule merge_contigs:
    input:   "{path}/clustering_refine.csv",
    output:  "{path}/clustering_gt%d_merged.csv" % MIN_CONTIG_SIZE
    log:     "{path}/consensus.log"
    threads: THREADS
    shell:   "{SCRIPTS}/Consensus.py {input} >{output} 2>{log}"

rule create_bin_folders:
    input:   bin="binning/clustering_gt%d_merged.csv" % MIN_CONTIG_SIZE,
             fasta="annotation/SCG.fna"
    output:  touch("subgraphs/bin_init/folder_done")
    shell:   "{SCRIPTS}/SCG_in_Bins.py {input.bin} {input.fasta} -f $(dirname {output})/"

rule compute_avg_cov:
    input:   "binning/clustering_gt%d_merged.csv" % MIN_CONTIG_SIZE
    output:  "subgraphs/bin_init/bin_cov.tsv"
    shell:   "{SCRIPTS}/bin_cov.py {input} {output} {ASSEMBLY_K}"

