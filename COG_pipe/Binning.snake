
#rule all_local: 
#   input: cov=expand("binning/{group}/C10K_coverage.tsv",group=GROUPS),
#          bins=expand("binning/{group}/clustering_gt500.csv",group=GROUPS)
#          touch=expand("binning/{group}/bin_creation.done",group=GROUPS)


#cut contigs by ORFs and get the bed file
rule cut_contigs:
    input:  fa="assembly/%s/{group}.fasta" % ASSEMBLER,
            gff="annotation/{group}/{group}.gff"
    output: contig="profile/split_c10k/{group}.fasta",
            Contig_bed="annotation/{group}/{group}_C10K_contig.bed"
    shell:  "{SCRIPTS}/Use_orf_to_cut.py {input.fa} {input.gff} {output.Contig_bed}> {output.contig}"


rule index_samples_bam:
    input:   "profile/{frags}/{group}/{sample}.sorted.bam",
    output:  "profile/{frags}/{group}/{sample}.sorted.bam.bai",
    message: "Indexing each bam file before coverage calcul"
    threads: THREADS
    shell:   "samtools index {input}"


# ---- use bedtool to compute coverage  ----------------------------------------------------
rule bedtools:
    input:   sample="profile/scaffolds/{group}/{sample}.sorted.bam",
             bed="annotation/{group}/{group}_C10K_contig.bed"
    output:  cov="profile/scaffolds/{group}/{sample}.cov",
    log:      "profile/scaffolds/{group}/{sample}.log"
    shell:   "bedtools coverage -a {input.bed} -b {input.sample} -mean > {output} 2>{log} "

# ---- use a awk onliner to regroup all coverages into a unique file -----------------------
rule coverage:
    input:   lambda w : ["profile/scaffolds/"+w.group+"/"+sample.split('/')[-1]+".cov" for sample in  GROUPS[w.group]]
    output:  "binning/{group}/Bin_ini/C10K_coverage.tsv"
    shell :  """
            echo -e "contig\t""$(ls {input} | cut -f1 -d "." | rev | cut -f1 -d "/" |rev | tr "\n" "\t" | sed 's/\t$//')"> {output}
            awk 'NR==FNR{{Matrix_coverage[1,FNR]=$4}}FNR==1{{f++}}{{Matrix_coverage[f+1,FNR]=$5}}END{{for(x=1;x<=FNR;x++){{for(y=1;y<ARGC+1;y++){{if(y<ARGC){{printf("%s\t",Matrix_coverage[y,x])}}if(y==ARGC){{printf("%s",Matrix_coverage[y,x]);print""}}}}}}}}' {input} >>{output}"""




rule initial_quantity_of_bins:
    # we take a look at all the SCG, take the median over the 36 of them, and multiply that by 10
    input: "annotation/{group}/{group}_SCG.fna"
    output:"annotation/{group}/{group}_nb_ini_bins.txt"
    shell: "{SCRIPTS}/get_num_bin_ini.py {input}>{output}"

#  concoct 
rule concoct:
    input:   cov="binning/{group}/Bin_ini/C10K_coverage.tsv",
             fasta="profile/split_c10k/{group}.fasta",
             nb_bin="annotation/{group}/{group}_nb_ini_bins.txt"
    output:  bins="binning/{group}/Bin_ini/clustering_gt%d.csv"%MIN_CONTIG_SIZE,
             Data="binning/{group}/Bin_ini/original_data_gt%d.csv"%MIN_CONTIG_SIZE             
    params:  mine_contig_size=MIN_CONTIG_SIZE
    log :   "binning/{group}/Bin_ini/concoct.logs"
    threads: 1000
    shell:   """
             concoct --coverage_file {input.cov} --composition_file {input.fasta} -b binning/{wildcards.group}/Bin_ini -c $(<{input.nb_bin}) -l {params} -t {threads} &>{log} 
             """


rule refine:
    input:  bins="binning/{group}/Bin_ini/clustering_gt%d.csv"%MIN_CONTIG_SIZE,
            SCG="annotation/{group}/{group}_SCG.fna",
            Data="binning/{group}/Bin_ini/original_data_gt%d.csv"%MIN_CONTIG_SIZE
    output: R=temp("binning/{group}/Bin_ini/clustering_gt%dR.csv")%MIN_CONTIG_SIZE,
            table="binning/{group}/Bin_ini/clustering_gt%d_SCG_table.csv"%MIN_CONTIG_SIZE,
            bins_R="binning/{group}/Bin_ini/clustering_refine.csv",
            table_R="binning/{group}/Bin_ini/clustering_gt%d_SCG_table_R.csv"%MIN_CONTIG_SIZE
    log:    temp("binning/{group}/Bin_ini/clustering.log")
    threads: 1000
    shell:  """ {SCRIPTS}/SCG_in_Bins.py {input.bins} {input.SCG} -t {output.table}
            sed '1d' {input.bins}  > {output.R}
            cd binning/{wildcards.group}/Bin_ini/
            concoct_refine ../../../{output.R} ../../../{input.Data} ../../../{output.table} -t {threads} &>../../../{log}
            cd ../../../
            {SCRIPTS}/SCG_in_Bins.py {output.bins_R} {input.SCG} -t {output.table_R}
            """

rule merge_contigs:
    input:   "{path}/clustering_refine.csv",
    output:  "{path}/clustering_gt"+str(MIN_CONTIG_SIZE)+"_merged.csv"
    log:     "{path}/consensus.log"
    threads: THREADS
    shell:   "{SCRIPTS}/Consensus.py {input} >{output} 2>{log}"

rule create_bin_folders:
    input:   bin="binning/{group}/Bin_ini/clustering_gt"+str(MIN_CONTIG_SIZE)+"_merged.csv",
             fasta="annotation/{group}/{group}_SCG.fna"
    output:  touch("binning/{group}/Bin_ini/folder_done")
    shell:   "{SCRIPTS}/SCG_in_Bins.py {input.bin} {input.fasta} -f binning/{wildcards.group}/Bin_ini/"

rule compute_avg_cov:
    input:   "binning/{group}/Bin_ini/clustering_gt%d_merged.csv" % MIN_CONTIG_SIZE
    output:  "binning/{group}/Bin_ini/bin_cov.tsv"
    shell:   "{SCRIPTS}/bin_cov.py {input} {output} {ASSEMBLY_K}"





